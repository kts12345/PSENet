{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # for flush\n",
    "import torch # for from _numpy, sigmoid, sum, save, load, optim.SGD\n",
    "import argparse # for ArgumentParser\n",
    "import numpy as np # for sum, sort, concatenate, int32 \n",
    "import torch.nn as nn # for DataParallel\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data # for DataLoader\n",
    "import os # for path, makedirs\n",
    "\n",
    "from dataset import IC15Loader\n",
    "from metrics import runningScore\n",
    "import models\n",
    "from util import Logger, AverageMeter\n",
    "\n",
    "import time # for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohem_single(score, gt_text, training_mask):\n",
    "    pos_num = (int)(np.sum(gt_text > 0.5)) - (int)(np.sum((gt_text > 0.5) & (training_mask <= 0.5)))\n",
    "    \n",
    "    if pos_num == 0:\n",
    "        # selected_mask = gt_text.copy() * 0 # may be not good\n",
    "        selected_mask = training_mask\n",
    "        selected_mask = selected_mask.reshape(1, selected_mask.shape[0], selected_mask.shape[1]).astype('float32')\n",
    "        return selected_mask\n",
    "    \n",
    "    neg_num = (int)(np.sum(gt_text <= 0.5))\n",
    "    neg_num = (int)(min(pos_num * 3, neg_num))\n",
    "    \n",
    "    if neg_num == 0:\n",
    "        selected_mask = training_mask\n",
    "        selected_mask = selected_mask.reshape(1, selected_mask.shape[0], selected_mask.shape[1]).astype('float32')\n",
    "        return selected_mask\n",
    "\n",
    "    neg_score = score[gt_text <= 0.5]\n",
    "    neg_score_sorted = np.sort(-neg_score)\n",
    "    threshold = -neg_score_sorted[neg_num - 1]\n",
    "\n",
    "    selected_mask = ((score >= threshold) | (gt_text > 0.5)) & (training_mask > 0.5)\n",
    "    selected_mask = selected_mask.reshape(1, selected_mask.shape[0], selected_mask.shape[1]).astype('float32')\n",
    "    return selected_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohem_batch(scores, gt_texts, training_masks):\n",
    "    scores = scores.data.cpu().numpy()\n",
    "    gt_texts = gt_texts.data.cpu().numpy()\n",
    "    training_masks = training_masks.data.cpu().numpy()\n",
    "    selected_masks = []\n",
    "    for i in range(scores.shape[0]):\n",
    "        selected_masks.append(ohem_single(scores[i, :, :], gt_texts[i, :, :], training_masks[i, :, :]))\n",
    "    selected_masks = np.concatenate(selected_masks, 0)\n",
    "    selected_masks = torch.from_numpy(selected_masks).float()\n",
    "    return selected_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target, mask):\n",
    "    input = torch.sigmoid(input)\n",
    "\n",
    "    input = input.contiguous().view(input.size()[0], -1)\n",
    "    target = target.contiguous().view(target.size()[0], -1)\n",
    "    mask = mask.contiguous().view(mask.size()[0], -1)\n",
    "    \n",
    "    input = input * mask\n",
    "    target = target * mask\n",
    "\n",
    "    a = torch.sum(input * target, 1)\n",
    "    b = torch.sum(input * input, 1) + 0.001\n",
    "    c = torch.sum(target * target, 1) + 0.001\n",
    "    d = (2 * a) / (b + c)\n",
    "    dice_loss = torch.mean(d)\n",
    "    return 1 - dice_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_text_score(texts, gt_texts, training_masks, running_metric_text):\n",
    "    training_masks = training_masks.data.cpu().numpy()\n",
    "    pred_text = torch.sigmoid(texts).data.cpu().numpy() * training_masks\n",
    "    pred_text[pred_text <= 0.5] = 0\n",
    "    pred_text[pred_text >  0.5] = 1\n",
    "    pred_text = pred_text.astype(np.int32)\n",
    "    gt_text = gt_texts.data.cpu().numpy() * training_masks\n",
    "    gt_text = gt_text.astype(np.int32)\n",
    "    running_metric_text.update(gt_text, pred_text)\n",
    "    score_text, _ = running_metric_text.get_scores()\n",
    "    return score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_kernel_score(kernels, gt_kernels, gt_texts, training_masks, running_metric_kernel):\n",
    "    mask = (gt_texts * training_masks).data.cpu().numpy()\n",
    "    kernel = kernels[:, -1, :, :]\n",
    "    gt_kernel = gt_kernels[:, -1, :, :]\n",
    "    pred_kernel = torch.sigmoid(kernel).data.cpu().numpy()\n",
    "    pred_kernel[pred_kernel <= 0.5] = 0\n",
    "    pred_kernel[pred_kernel >  0.5] = 1\n",
    "    pred_kernel = (pred_kernel * mask).astype(np.int32)\n",
    "    gt_kernel = gt_kernel.data.cpu().numpy()\n",
    "    gt_kernel = (gt_kernel * mask).astype(np.int32)\n",
    "    running_metric_kernel.update(gt_kernel, pred_kernel)\n",
    "    score_kernel, _ = running_metric_kernel.get_scores()\n",
    "    return score_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    running_metric_text = runningScore(2)\n",
    "    running_metric_kernel = runningScore(2)\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (imgs, gt_texts, gt_kernels, training_masks) in enumerate(tqdm(train_loader)):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        imgs = Variable(imgs.cuda())\n",
    "        gt_texts = Variable(gt_texts.cuda())\n",
    "        gt_kernels = Variable(gt_kernels.cuda())\n",
    "        training_masks = Variable(training_masks.cuda())\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        texts   = outputs[:, 0, :, :]\n",
    "        kernels = outputs[:, 1:, :, :]\n",
    "\n",
    "        selected_masks = ohem_batch(texts, gt_texts, training_masks)\n",
    "        selected_masks = Variable(selected_masks.cuda())\n",
    "\n",
    "        loss_text = criterion(texts, gt_texts, selected_masks)\n",
    "        \n",
    "        loss_kernels = []\n",
    "        mask0 = torch.sigmoid(texts).data.cpu().numpy()\n",
    "        mask1 = training_masks.data.cpu().numpy()\n",
    "        selected_masks = ((mask0 > 0.5) & (mask1 > 0.5)).astype('float32')\n",
    "        selected_masks = torch.from_numpy(selected_masks).float()\n",
    "        selected_masks = Variable(selected_masks.cuda())\n",
    "        for i in range(6):\n",
    "            kernel_i = kernels[:, i, :, :]\n",
    "            gt_kernel_i = gt_kernels[:, i, :, :]\n",
    "            loss_kernel_i = criterion(kernel_i, gt_kernel_i, selected_masks)\n",
    "            loss_kernels.append(loss_kernel_i)\n",
    "        loss_kernel = sum(loss_kernels) / len(loss_kernels)\n",
    "        \n",
    "        loss = 0.7 * loss_text + 0.3 * loss_kernel\n",
    "        losses.update(loss.item(), imgs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        score_text   = cal_text_score(texts, gt_texts, training_masks, running_metric_text)\n",
    "        score_kernel = cal_kernel_score(kernels, gt_kernels, gt_texts, training_masks, running_metric_kernel)\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            output_log  = '({batch}/{size}) Batch: {bt:.3f}s | TOTAL: {total:.0f}min | ETA: {eta:.0f}min | Loss: {loss:.4f} | Acc_t: {acc: .4f} | IOU_t: {iou_t: .4f} | IOU_k: {iou_k: .4f}'.format(\n",
    "                batch=batch_idx + 1,\n",
    "                size=len(train_loader),\n",
    "                bt=batch_time.avg,\n",
    "                total=batch_time.avg * batch_idx / 60.0,\n",
    "                eta=batch_time.avg * (len(train_loader) - batch_idx) / 60.0,\n",
    "                loss=losses.avg,\n",
    "                acc=score_text['Mean Acc'],\n",
    "                iou_t=score_text['Mean IoU'],\n",
    "                iou_k=score_kernel['Mean IoU'])\n",
    "            print(output_log)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return (losses.avg, score_text['Mean Acc'], score_kernel['Mean Acc'], score_text['Mean IoU'], score_kernel['Mean IoU'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(args, optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in args.schedule:\n",
    "        args.lr = args.lr * 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr\n",
    "\n",
    "def save_checkpoint(state, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.checkpoint == '':\n",
    "        args.checkpoint = \"checkpoints/ic15_%s_bs_%d_ep_%d\"%(args.arch, args.batch_size, args.n_epoch)\n",
    "    if args.pretrain:\n",
    "        if 'synth' in args.pretrain:\n",
    "            args.checkpoint += \"_pretrain_synth\"\n",
    "        else:\n",
    "            args.checkpoint += \"_pretrain_ic17\"\n",
    "\n",
    "    print(('checkpoint path: %s'%args.checkpoint))\n",
    "    print(('init lr: %.8f'%args.lr))\n",
    "    print(('schedule: ', args.schedule))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.isdir(args.checkpoint):\n",
    "        os.makedirs(args.checkpoint)\n",
    "\n",
    "    kernel_num = 7\n",
    "    min_scale = 0.4\n",
    "    start_epoch = 0\n",
    "\n",
    "    data_loader = IC15Loader(is_transform=True, img_size=args.img_size, kernel_num=kernel_num, min_scale=min_scale)\n",
    "    train_loader = data.DataLoader(\n",
    "        data_loader,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=12,\n",
    "        drop_last=True,\n",
    "        pin_memory=False)\n",
    "\n",
    "    if args.arch == \"resnet50\":\n",
    "        model = models.resnet50(pretrained=True, num_classes=kernel_num)\n",
    "    elif args.arch == \"resnet101\":\n",
    "        model = models.resnet101(pretrained=True, num_classes=kernel_num)\n",
    "    elif args.arch == \"resnet152\":\n",
    "        model = models.resnet152(pretrained=True, num_classes=kernel_num)\n",
    "    \n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    \n",
    "    if hasattr(model.module, 'optimizer'):\n",
    "        optimizer = model.module.optimizer\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.99, weight_decay=5e-4)\n",
    "\n",
    "    title = 'icdar2015'\n",
    "    if args.pretrain:\n",
    "        print('Using pretrained model.')\n",
    "        assert os.path.isfile(args.pretrain), 'Error: no checkpoint directory found!'\n",
    "        checkpoint = torch.load(args.pretrain)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title)\n",
    "        logger.set_names(['Learning Rate', 'Train Loss','Train Acc.', 'Train IOU.'])\n",
    "    elif args.resume:\n",
    "        print('Resuming from checkpoint.')\n",
    "        assert os.path.isfile(args.resume), 'Error: no checkpoint directory found!'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title, resume=True)\n",
    "    else:\n",
    "        print('Training from scratch.')\n",
    "        logger = Logger(os.path.join(args.checkpoint, 'log.txt'), title=title)\n",
    "        logger.set_names(['Learning Rate', 'Train Loss','Train Acc.', 'Train IOU.'])\n",
    "\n",
    "    for epoch in range(start_epoch, args.n_epoch):\n",
    "        adjust_learning_rate(args, optimizer, epoch)\n",
    "        print(('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.n_epoch, optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        train_loss, train_te_acc, train_ke_acc, train_te_iou, train_ke_iou = train(train_loader, model, dice_loss, optimizer, epoch)\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'lr': args.lr,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, checkpoint=args.checkpoint)\n",
    "\n",
    "        logger.append([optimizer.param_groups[0]['lr'], train_loss, train_te_acc, train_te_iou])\n",
    "    logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint path: checkpoints/ic15_resnet50_bs_12_ep_600\n",
      "init lr: 0.00100000\n",
      "('schedule: ', [200, 400])\n",
      "Training from scratch.\n",
      "\n",
      "Epoch: [1 | 600] LR: 0.001000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f651fbf2edb547ebb905def2fd38bf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=833), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kts123/miniconda3/envs/main/lib/python3.6/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/kts123/miniconda3/envs/main/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/833) Batch: 12.224s | TOTAL: 0min | ETA: 170min | Loss: 0.6339 | Acc_t:  0.5222 | IOU_t:  0.3634 | IOU_k:  0.7568\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main_(): \n",
    "    parser = argparse.ArgumentParser(description='Hyperparams')\n",
    "    parser.add_argument('--arch', nargs='?', type=str, default='resnet50')\n",
    "    parser.add_argument('--img_size', nargs='?', type=int, default=640, \n",
    "                        help='Height of the input image')\n",
    "    parser.add_argument('--n_epoch', nargs='?', type=int, default=600, \n",
    "                        help='# of the epochs')\n",
    "    parser.add_argument('--schedule', type=int, nargs='+', default=[200, 400],\n",
    "                        help='Decrease learning rate at these epochs.')\n",
    "    parser.add_argument('--batch_size', nargs='?', type=int, default=12, \n",
    "                        help='Batch Size')\n",
    "    parser.add_argument('--lr', nargs='?', type=float, default=1e-3, \n",
    "                        help='Learning Rate')\n",
    "    parser.add_argument('--resume', nargs='?', type=str, default=None,    \n",
    "                        help='Path to previous saved model to restart from')\n",
    "    parser.add_argument('--pretrain', nargs='?', type=str, default=None,    \n",
    "                        help='Path to previous saved model to restart from')\n",
    "    parser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n",
    "                    help='path to save checkpoint (default: checkpoint)')\n",
    "    \n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    main(args)\n",
    "\n",
    "main_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
